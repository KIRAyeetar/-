# "阿里移动推荐算法" 工程记录


- #### [工程简介](#工程简介)

  - ##### [基本信息](#基本信息)

  - ##### [赛题介绍](#赛题介绍)
    - ###### [赛题背景](#赛题背景)
    - ###### [赛题数据](#赛题数据)
    - ###### [评估相关](#评估相关)

  - ##### [工程人员名单](#工程人员名单)

- #### [详细记录](#详细记录)
  - ##### [工程日记](#工程日记)
  - ##### [提交记录](#提交记录)




## 工程简介

#### 基本信息

​	本工程创建于2019/11/21，最后更新于2019/11/26，用于针对 [天池新人实战赛之[离线赛] - 阿里移动推荐算法](https://tianchi.aliyun.com/competition/entrance/231522/introduction) 赛题进行数据处理、特征工程、构建模型等步骤完成预测算法。该题属于 __广告推送 __ 型的 __多分类__ 赛题，评价指标为 __F1__ 值。



#### 赛题介绍

##### 赛题背景

​	[阿里移动推荐算法背景](https://tianchi.aliyun.com/competition/entrance/231522/introduction)：赛题以阿里巴巴移动电商平台的真实用户-商品行为数据为基础，同时提供移动时代特有的位置信息，而参赛队伍则需要通过大数据和算法构面向建移动电子商务的商品推荐模型。希望参赛队伍能够挖掘数据背后丰富的内涵，为移动用户在合适的时间、合适的地点精准推荐合适的内容。	

​	在真实的业务场景下，我们往往需要对所有商品的一个子集构建个性化推荐模型。在完成这件任务的过程中，我们不仅需要利用用户在这个商品子集上的行为数据，往往还需要利用更丰富的用户行为数据。定义如下的符号： ```U——用户集合``` ```I——商品全集``` ```P——商品子集, P⊆I``` ```D——用户对商品全集的行为数据集合``` ，那么我们的目标是利用D来构造U中用户对P中商品的购买推荐模型。 

##### 赛题数据

​	赛题原数据大小（解压缩后）约为999M，包括以下两个文件及格式：

​       ```tianchi_fresh_comp_train_item.csv```

| 字段    | 字段说明 | 提取说明      |
| :------ | :------- | :------------ |
| item_id | 商品标识 | 抽样&字段脱敏 |
| item_geohash | 商品位置的空间标识，可以为空 | 由经纬度通过保密的算法生成 |
| item_category | 商品分类标识 | 字段脱敏 |

 	```tianchi_fresh_comp_train_user.csv```
| 字段    | 字段说明 | 提取说明      |
| :------ | :------- | :------------ |
| user_id | 用户标识 | 抽样&字段脱敏 |
| item_id | 商品标识 | 抽样&字段脱敏 |
| behavior_type | 用户对商品的行为类型 | 包括浏览、收藏、加购物车、购买，对应取值分别是1、2、3、4 |
| user_geohash | 用户对商品的行为类型 | 由经纬度通过保密的算法生成 |
| item_category | 商品分类标识 | 字段脱敏 |
| time | 行为时间 | 精确到小时级别 |

##### 评估相关

​	评分数据格式具体计算公式如下：参赛者完成用户对商品子集的购买预测之后，需要将结果放入指定格式的数据表（非分区表）中，要求结果表名为： ```tianchi_mobile_recommendation_predict.csv``` ，且以utf-8格式编码；包含user_id和item_id两列（均为string类型），要求去除重复。

​	最后评估采用 __F1值__ 为评估指标。

```
Precision = True Positive / (True Positive + False Positive)
recall = True Positive / (True Positive + False Negative)
F1 = (2 * Precision * recall) / (Precision + recall)
```



#### 工程人员名单

| ID       | Email            | Github                |
| :------- | :--------------- | :-------------------- |
| 严武小虎 | 424276284@qq.com | github.com/KIRAyeetar |




## 详细记录

#### 工程日记

```
2019/11/24 及之前
```

> 完成工程代码结构的初步设计；
>
> 完成文件 ```ComP/Main/A_Read＆Processing_a/01_read.py``` 读入原数据的功能；
>
> 完善文件 ```ComP/Main/A_Read＆Processing_a/02_analyze.py``` 的数据分析和画图分析功能；

```
2019/11/25 01:49
```

> 完成该《工程报告》的基本设计，顺便 ~~学习~~ 复习了Markdown的一些语法。

```
2019/11/26 00:21
```

> 新建下列文件，之后几天准备做一个初步的试错模型。 
> ```F:\pyCodes\ComP\Main\B_FeatureEngineering_a\01_split＆label.py```
> ```F:\pyCodes\ComP\Main\B_FeatureEngineering_a\02_feature-getting.py```
> ```F:\pyCodes\ComP\Main\C_Model＆Ensemble_a\01_xgboost.py```
> ```F:\pyCodes\ComP\Main\C_Model＆Ensemble_a\02_LR.py``` 
> ```F:\pyCodes\ComP\Main\C_Model＆Ensemble_a\03_linear_weighting.py```
>
> 试错的基本思路：以“一周”为单位划分数据，提取基本的计数、求和、求均值等特征，最后用xgb和lr线性加权融合得到一个结果。
>
> -----
> 下面是一些之前画的图的分析：
>
> ![img](F:\pyCodes\ComP\Data\Graph\sale_count_by_week.jpg)
>
> 上图为以周为单位的销售量图，除了异常天数看不出太多东西。
>
> ![img](F:\pyCodes\ComP\Data\Graph\item_sale_by_day_rank.jpg)
>
> 上图为连续31天的销售量图，特点不是太明显。
>
> ![img](F:\pyCodes\ComP\Data\Graph\log_count_by_day_rank.jpg)
>
> 上图为连续31天的记录图，得出 __大约周五前后为周期性波谷__（除去异常天数）。
>
> ![img](F:\pyCodes\ComP\Data\Graph\item_count_by_day_rank.jpg)
>
> 上图为连续31天的商品数量（多少种商品），得出 __周五商品为最少的一天__（除去异常天数）。
>
> ![img](F:\pyCodes\ComP\Data\Graph\user_count_by_day_rank.jpg)
>
> 上图为连续31天的用户量图，得出 __大约周五前后为周期性波谷__（除去异常天数）。
>
> 而我们要预测的下一天，正好也是周五，所以可以根据这种周期性规律做点文章。问题是异常天数刚好也在周五，反而销售量最高，说明那两天有某些活动刺激了消费，才出现异常。由此可知一个难点是怎么处理异常天数。
>
> ![img](F:\pyCodes\ComP\Data\Graph\item_sale_by_item.jpg)
>
> 上图为销售量前100（总共约400W种）的商品的总销售量，可以看出有一个商品销售量极高，有极高可能性被购买；另外的都是低于2000以下的销售量。但其余非常多都是销量接近1的运气选手，因为总和销量24W，与上图前100销量的直观感受不符。
>
> ![img](F:\pyCodes\ComP\Data\Graph\item_sale_by_user.jpg)
>
> 上图为2W用户的购买总量，虽然很难看，但可以看出有极少数购买量极高，属于一人拉高纵坐标的石油佬。
>
> ![img](F:\pyCodes\ComP\Data\Graph\user_count_by_sale_day_count.jpg)
>
> 上图为31天内，顾客个人有购买行为的天数统计图。可以看做一个平稳的数据，说明顾客总体的购买习惯较之某些昙花一现的商品更具稳定性。横坐标摆的不太好。
>
> ----
>
> ​	总体而言，购买的周期性特征不是特别明显。目前除开试错模型，另外一个思路是：大环境规律决定总购买量（周五波谷+异常因素），用户数据决定个人购买能力和习惯，再结合商品数据决定购买偏好。再简略的说就是两个方向：先看你买不买，再看你买什么。买不买的也有两个因素：一是本题中的异常天数是否影响你下个周五的购买能力，二是你个人有没有意向或者习惯买。前者目前考虑用LSTM，以一天为序列学习；后者用正常的机器学习模型，数据集划分方法不确定。而买什么的问题，应该是正常机器学习方法，结合用户习惯和商品数据去做。
>
> ​	最后一个大问题是多分类怎么做，4W多的商品不可能用40W个标签或者40W个模型一个一个的得出结果，这应该是之后考虑的重点。
>
```
2019/11/26 13:12
```

> 准备研究下经纬度脱敏的问题。即使是脱敏，那应该也是保留了一定的信息量，比如距离关系，否则单独列出经纬度这个数据根本没必要。所以得想想办法解码为正常地球人看得懂的信息。根据脱敏数据的长相，目前猜测是Geohash， [~~但是在安装相关包的时候碰到点问题，正在解决~~。](https://blog.csdn.net/kdyyh/article/details/73694482) 但尝试失败，发现不是python原包中的geohash编码。
>
> 研究下 [geohash原理](http://sighsmile.github.io/2017-06-30-geohash-python/) 可知，geohash使用一种base32的编码方式，字符集只有大写字母（A-Z）和数字234567；而geohash中对字符集做了修改，只包含除去a,i,l,o的小写字母和数字0123456789 。
>
> ```geohash中的base32字符集: 0123456789bcdefghjkmnpqrstuvwxyz```
>
> ```常见的base32字符集: ABCDEFGHIJKLMNOPQRSTUVWXYZ234567```
>
> 而原数据包含有像“96p6tvo”的格式包含有"o"，所以并不是geohash。
>
> 延伸base32编码的知识，还有 [base64的编码](https://blog.csdn.net/xx326664162/article/details/78122139) ，不包含上述base32的问题。
>
> ```原的base64字符集:ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/```
>
> 可原数据并无大写字母，所以暂时认为是将base32的字符集进行修改的能解码方法。
>
> 之后会尝试。

```
2019/11/26 16:41
```

> 收集经纬度中的所有字符，再清洗一下，得到如下字符set：
>
> ```经纬度数据中字符set：012345679abcdefghijklmnopqrstuvw```
>
> 易得缺少了8,x,y,z。
>
> gui cai believe 刚好32位是巧合，所以就这样把上面32位将原Geohash包中的字符替换，成为经纬度解码工具```F:\pyCodes\ComP\Tools\geohash32.py```

```
2019/11/26 23:14
```

> 报！发现重大BUG，在此之前把所有记录都当做了“购买”行为，然而记录包括了4种行为（浏览、收藏、加入购物车、购买）。接下来修改错误，特别是 ```analyze``` 中的分析... ... 我裂开了，一个BUG改**半天... ...
>
> -----
>
> 新建了数据清洗文件，输出了处理经纬度和时间的CSV文件，用于之后的特征工程。```ComP/Main/A_Read＆Processing_a/03_clean.py```

```
2019/12/08 00:08
```
> 创建```ComP/Tools/f1_specail.py``` 用于特殊计算该赛题的f1值。
>
> 创建文件夹 ```ComP/Data/Csv/ClnData/a``` 用于存放a组方案的已清洗数据。
>
> --------------
>
> 思考了一下之后的多分类怎么做，一种简单的办法是只留最有可能被购买的几种商品当标签，直接进行多分类学习。然而这样的缺点是有可能遗失掉冷门的商品被购买的概率，优点嘛就是简单而已。暂定用该种方案做出a组预测。

#### 提交记录

暂无
