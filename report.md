# "阿里移动推荐算法" 工程记录


- [工程简介](#工程简介)
  - [基本信息](#基本信息)
  - [赛题介绍](#赛题介绍)
    - [赛题背景](#赛题背景)
    - [赛题数据](#赛题数据)
    - [评估相关](#评估相关)
  - [工程人员名单](#工程人员名单)

- [详细记录](#详细记录)
  - [工程日记](#工程日记)
  - [提交记录](#提交记录)

- [方案保存](#参考文献) 
  - [特征方案](#赛题数据)
  - [模型参数](#评估相关)
  - [验证集基准](#验证集基准)

-  [参考文献](#参考文献)



## 工程简介

#### 基本信息

​	本工程创建于2019/11/21，用于针对 [天池新人实战赛之[离线赛] - 阿里移动推荐算法](https://tianchi.aliyun.com/competition/entrance/231522/introduction) 赛题进行数据处理、特征工程、构建模型等步骤完成预测算法。该题属于 __广告推送 __ 型的 __多标签__ 赛题，评价指标为 __F1__ 值。



#### 赛题介绍

##### 赛题背景

​	[阿里移动推荐算法背景](https://tianchi.aliyun.com/competition/entrance/231522/introduction)：赛题以阿里巴巴移动电商平台的真实用户-商品行为数据为基础，同时提供移动时代特有的位置信息，而参赛队伍则需要通过大数据和算法构面向建移动电子商务的商品推荐模型。希望参赛队伍能够挖掘数据背后丰富的内涵，为移动用户在合适的时间、合适的地点精准推荐合适的内容。	

​	在真实的业务场景下，我们往往需要对所有商品的一个子集构建个性化推荐模型。在完成这件任务的过程中，我们不仅需要利用用户在这个商品子集上的行为数据，往往还需要利用更丰富的用户行为数据。定义如下的符号： ```U——用户集合``` ```I——商品全集``` ```P——商品子集, P⊆I``` ```D——用户对商品全集的行为数据集合``` ，那么我们的目标是利用D来构造U中用户对P中商品的购买推荐模型。 

##### 赛题数据

  赛题原数据大小（解压缩后）约为999M，包括以下两个文件及格式：

  用户对商品全集行为表：`tianchi_fresh_comp_train_item.csv`

| 字段    | 字段说明 | 提取说明      |
| :------ | :------- | :------------ |
| item_id | 商品标识 | 抽样&字段脱敏 |
| item_geohash | 商品位置的空间标识，可以为空 | 由经纬度通过保密的算法生成 |
| item_category | 商品分类标识 | 字段脱敏 |

  商品子集信息表：`tianchi_fresh_comp_train_user.csv`
| 字段    | 字段说明 | 提取说明      |
| :------ | :------- | :------------ |
| user_id | 用户标识 | 抽样&字段脱敏 |
| item_id | 商品标识 | 抽样&字段脱敏 |
| behavior_type | 用户对商品的行为类型 | 包括浏览、收藏、加购物车、购买，对应取值分别是1、2、3、4 |
| user_geohash | 用户对商品的行为类型 | 由经纬度通过保密的算法生成 |
| item_category | 商品分类标识 | 字段脱敏 |
| time | 行为时间 | 精确到小时级别 |

##### 评估相关

​	评分数据格式具体计算公式如下：参赛者完成用户对商品子集的购买预测之后，需要将结果放入指定格式的数据表（非分区表）中，要求结果表名为： ```tianchi_mobile_recommendation_predict.csv``` ，且以utf-8格式编码；包含user_id和item_id两列（均为string类型），要求去除重复。

​	最后评估采用 __F1值__ 为评估指标。
```
Precision = True Positive / (True Positive + False Positive)
recall = True Positive / (True Positive + False Negative)
F1 = (2 * Precision * recall) / (Precision + recall)
```



#### 工程人员名单

| ID       | Email            | Github                |
| :------- | :--------------- | :-------------------- |
| 严武小虎 | 424276284@qq.com | github.com/KIRAyeetar |




## 详细记录

#### 工程日记

```
2019/11/24 及之前 —严武小虎
```

> 完成工程代码结构的初步设计；
>

```
2019/11/25 01:49
```

> 完成该《工程报告》的基本设计，顺便 ~~学习~~ 复习了Markdown的一些语法。

```
2019/11/26 00:21
```

> 之后准备做一个初步的试错模型。 
> 
> 试错的基本思路：以“一周”为单位划分数据，提取基本的计数、求和、求均值等特征，最后用xgb和lr线性加权融合得到一个结果。
> 
> 根据分析而言，数据的周期性是有，但不是很铭心啊特别明显。目前除开试错模型，另外一个思路是：大环境规律决定总购买量（周五波谷+异常因素），用户数据决定个人购买能力和习惯，再结合商品数据决定购买偏好。再简略的说就是两个方向：先看你买不买，再看你买什么。买不买的也有两个因素：一是本题中的异常天数是否影响你下个周五的购买能力，二是你个人有没有意向或者习惯买。前者目前考虑用LSTM，以一天为序列学习；后者用正常的机器学习模型，数据集划分方法不确定。而买什么的问题，应该是正常机器学习方法，结合用户习惯和商品数据去做。
> 
>​	最后一个大问题是多标签怎么做，4W多的商品不可能用40W个标签或者40W个模型一个一个的得出结果，这应该是之后考虑的重点。
> 
```
2019/11/26 13:12
```

> 准备研究下经纬度脱敏的问题。即使是脱敏，那应该也是保留了一定的信息量，比如距离关系，否则单独列出经纬度这个数据根本没必要。所以得想想办法解码为正常地球人看得懂的信息。根据脱敏数据的长相，目前猜测是Geohash， [~~但是在安装相关包的时候碰到点问题，正在解决~~。](https://blog.csdn.net/kdyyh/article/details/73694482)<sup>[1]</sup> 但尝试失败，发现不是python原包中的geohash编码。
>
> 研究下 [geohash原理](http://sighsmile.github.io/2017-06-30-geohash-python/)<sup>[2]</sup> 可知，geohash使用一种base32的编码方式，字符集只有大写字母（A-Z）和数字234567；而geohash中对字符集做了修改，只包含除去a,i,l,o的小写字母和数字0123456789 。
>
> ```geohash中的base32字符集: 0123456789bcdefghjkmnpqrstuvwxyz```
>
> ```常见的base32字符集: ABCDEFGHIJKLMNOPQRSTUVWXYZ234567```
>
> 而原数据包含有像“96p6tvo”的格式包含有"o"，所以并不是geohash。
>
> 延伸base32编码的知识，还有 [base64的编码](https://blog.csdn.net/xx326664162/article/details/78122139)<sup>[3]</sup> ，不包含上述base32的问题。
>
> ```原的base64字符集:ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/```
>
> 可原数据并无大写字母，所以暂时认为是将base32的字符集进行修改的能解码方法。
>
> 之后会尝试。

```
2019/11/26 16:41
```

> 收集经纬度中的所有字符，再清洗一下，得到如下字符set：
>
> ```经纬度数据中字符set：012345679abcdefghijklmnopqrstuvw```
>
> 易得缺少了8,x,y,z。
>
> gui cai believe 刚好32位是巧合，所以就这样把上面32位将原Geohash包中的字符替换，成为经纬度解码工具```F:\pyCodes\ComP\Tools\geohash32.py```

```
2019/11/26 23:14
```

> 报！发现重大BUG，在此之前把所有记录都当做了“购买”行为，然而记录包括了4种行为（浏览、收藏、加入购物车、购买）。接下来修改错误，特别是 ```analyze``` 中的分析... ... 我裂开了，一个BUG改**半天... ...
>
> -----
>
> 新建了数据清洗文件，输出了处理经纬度和时间的CSV文件，用于之后的特征工程。

```
2019/12/08 00:08
```
> 思考了一下之后的多 ~~分类~~ 标签怎么做，一种简单的办法是只留最有可能被购买的几种商品当标签，直接进行多 ~~分类~~ 标签学习。然而这样的缺点是有可能遗失掉冷门的商品被购买的概率，优点嘛就是简单而已。暂定用该种方案做出a组预测。
>
> -----
>
> 在github上创建了赛题的新仓库，虽然只上传了这个Report，但发现锚点失效了，我裂开了... ... 以后有空再修。

```
2019/12/14 20:00
```

> 修改工程结构并为文件重命名，顺便实践一下新学习的 [markdown  mermaid 语法](https://blog.csdn.net/xc_oo0/article/details/97126647)<sup>[4]</sup>
> ```mermaid
> graph LR
> id4(阿里移动推荐算法工程) --> 数据文件夹
> id4(阿里移动推荐算法工程) --> 代码主文件
> id4(阿里移动推荐算法工程) --> 临时文件
> id4(阿里移动推荐算法工程) --> 自定义工具包
> 
> 数据文件夹 --> CSV文件
> 数据文件夹 --> 图文件 
> 
> CSV文件 --> 已清洗数据文件夹
> CSV文件 --> 特征文件夹 
> CSV文件 --> 结果文件夹 
> CSV文件 --> 原始文件夹 
> 
> 已清洗数据文件夹 -. 选择 .-> id1((方案组))
> 特征文件夹 -. 选择 .-> id1((方案组))
> 结果文件夹 -. 选择 .-> id1((方案组))
> 
> 代码主文件 --> 公用数据预处理 
> 代码主文件 -.选择.-> id2((方案组)) 
> 
> id2((方案组)) -.-> 特征工程代码组
> id2((方案组)) -.-> 模型设计代码组
> id2((方案组)) -.-> 特殊设计代码组
> id2((方案组)) -.-> 代码备份
> 
> 自定义工具包 --> 常用工具
> 自定义工具包 -.-> 工程专用工具
> ```

```
2019/12/15 02:35
```
>碰见麻烦事，原本的思路是取销量前几的商品做固定标签，然后做一个多分类模型。然而某些狗大户可能在同一天购买多种商品，这样的话单纯的多分类就会损失过大。莫非真的只能一个模型一件商品的去判断吗... 需要花时间思考一下。
>
>再查下 [XGBoost文档](https://xgboost.apachecn.org/#/docs/15)<sup>[5]</sup> 可知xgb可以做多标签，而且能调整参数  ```objective: multi:softprob``` 输出概率矩阵，这样的话我们还是可以自己定义阈值，一定程度上解决一人对多种商品的问题。
>
>另外一个还是深度学习模型，天生自带多 ~~分类~~ 标签易伤BUFF，还是得尝试一下。
>
>再另外，一件商品一个模型也可以试试，针对某些特殊情况的商品，说不定有奇效。当然只是奇效，不报太大希望。

```
2019/12/15 14:14
```

> 可以解决多标签的模型听说有 [深度森林](https://arxiv.org/abs/1911.06557)<sup>[6]</sup> ，来自于周志华老师，但实际效果不好说，只说竞赛圈，这个模型出现的次数不太多。另外一个还是深度学习模型。如果要用传统的机器学习模型，基本又只能回归 one vs all 这类的思路，太麻烦。

```
2019/12/15 17:51
```

> 我真的是火星了，Tensorflow现在是2.x的版本，而且不兼容之前1.x的代码... 所以要用深度学习还得重新学学 [tf2.0](https://tf.wiki/)<sup>[7]</sup> 。另外当Tensorflow版本大于等于1.13时，使用的cuda版本必须大于等于10.0，想用pip装1.13版本之前的tf时，你又会发现支持的版本刚好从 ```1.13.0 Orc1``` 起步... ... 啊，重新装一遍Tensorflow吧... ... 裂开了。早晚得入Pytorch。

```
2019/12/18 00:11
```

> 看数据玩时发现个BUG。原item列表里，同一个item可能对应多个经纬度，所以我在做总表的merge时是有问题的。现在看最好先把item的经纬度删去，再merge，之后提特征时再加回去。
>
> 另外还有一个问题，这个比赛是对特定的商品子集进行推荐，而非所有出现的商品，究竟是只拿子集做特征呢，还是把所有的都算上，需要进一步尝试。我目前希望是把商品全集的信息拿来做迁移学习，真正的预测还是靠子集的商品。

```
2019/12/26 23:18
```
> 今天某宝上冲Q币买一折皮肤，突然发现个可以用的规则——购物车。虽然我从来都是直接付款，但或许对于很多人而言都是会买添加在购物车里的东西。算法搞不出来，就先搞搞规则算了，就用最后一天加在购物车的记录直接交上去看看结果，甚至这有可能当做解决多标签的思路之一。
>
> 第一个思路是 把前一天在购物车列表的所有商品都交上去，上下分别为全集子集商品，结果如下并不好看 。
>
> ```
> 第28天为标签
> 0.04236068270614847
> 0.04067197170645446
> 第29天为标签
> 0.04103085791793828
> 0.045081967213114756
> 第30天为标签
> 0.04717457114026236
> 0.04757630161579893
> ```
> 
> ----
> 
> 还有一个测试，是满足上面条件的同时，又加上只取销售历史量在2及以上的商品，结果更惨。说明历史销量和扔进购物车之间关联并不大。甚至我还在想，用“加入购物车”的类似的即时行为来解决神经病购买，即毫无征兆的购买以前没买过的商品，再用一般的数据解决最大概率购买，即用算法预测的购买行为。
> 
> ```
> 第28天为标签
> 0.014276649746192893
> 0.018944519621109608
> 第29天为标签
> 0.014625797417146414
> 0.02469135802469136
> 第30天为标签
> 0.01789847245795402
> 0.01593625498007968
> ```
> 
> -----
> 
> 根据以上思路，再做一个马后炮的测试。假设我们预先知道正确标签，用曾出现在前一天的购物车里的正确商品为答案。新增的一行是按顺序统计预测和目标文件的行数。
> 
> 这分和网上别人的成绩比还可以，但是看这行数差太多，明显召回率太低了，说明有N多人不买东西不会在前一天加购物车，总之处理神经病购买还是有难度。
> 
> ```
>第28天为标签
> 515 4367
>0.21097910692339206
> 46 504
>0.16727272727272727
> 
> 第29天为标签
> 484 4429
> 0.19702829228577243
> 55 553
> 0.18092105263157893
> 
> 第30天为标签
> 561 4272
> 0.23215394165114833
> 53 515
> 0.18661971830985916
> ```
> 
> ------
> 
> 顺延上面的思路，假设预先知道正确标签，根据正确标签，用曾经收藏过的商品为答案。结果一般。
> 
> ```
> 第28天为标签
> 349 4367
> 0.14673452078032231
> 31 504
> 0.11588785046728971
> 
> 第29天为标签
> 388 4429
> 0.15818974465434918
> 42 553
> 0.1411764705882353
> 
> 第30天为标签
>353 4272
> 0.15091891891891893
>44 515
> 0.15742397137745973
>```
> ----
>
> 讲道理我们再思考下，这个赛题最大的问题之一就是多标签，必须得想办法把一用户对多商品的问题给解决了，上万个商品全部预测肯定不现实，目前只能想缩小预测范围。
> 
> 而又一个问题是什么，我们如果根据过去的行为预测未来，这在本赛题也不太理想，为什么？因为这是购物，假设我第一天买了手机，我第二天会再继续买手机吗？所以用过去预测未来，方向不能跑偏了，具体什么方向是对的也难说。比如缩小标签范围就会碰到这个问题，我昨天买了手机，那我今天把手机当成要预测的标签，这肯定是不现实的。
> 
> 所以一个用户应该拿满足什么条件的商品当自己的标签呢？实属难顶。
> 
> 按照上面用前一天购物车的想法，做到极致的话也行，但也是极致，所以保留这个想法，也就是说想想办法，找到前一天购物车里，用户一定会购买的一些商品的规律。按照这个思路，一个用户的标签范围就缩小到了自己前一天购物车里的商品，我们要的效果就有了。
> 
> 所以还可以找找其他类似的想法，争取F1值在马后炮的情况下做到更高，选择最高的方案再介入机器学习之类的。

``` 
2019/12/27 11:29
```
> 顺延昨天的思路，新测试一个 标签前一天所有在购物车且之前收藏过的商品。结果不咋样，说明是否收藏对用户买不买商品关系不够大，没达到预想的标准，不如直接选前一天购物车里的商品。
>
> ```
> 第28天为标签
> 4488 4367
> 0.027780914737436477
> 416 504
> 0.02826086956521739
> 第29天为标签
> 4570 4429
> 0.02689187687520836
> 469 553
> 0.0273972602739726
> 第30天为标签
> 4761 4272
> 0.03144027454887634
> 470 515
> 0.024365482233502538
> ```
> 
> -----
> 
> 还有一个没有测试过，什么？浏览记录，买东西总得先看吧，看得出来在准确率100%的情况下，召回率直线上升，牛逼。
> 
> ```
> 第28天为标签
> 1018 4367
> 0.3777158774373259
> 105 504
> 0.3448275862068966
> 
> 第29天为标签
> 1037 4429
> 0.37870472008781564
> 138 553
> 0.3994211287988423
> 
> 第30天为标签
> 1050 4272
> 0.39383690341976696
> 125 515
> 0.39062500000000006
> ```
> 
> --------
> 
> 这下一个思路就有了哦，每个用户的标签减少到他前一天浏览过的商品集，做到极致就是0.4左右的F1值，远超当前已知的最高分。
> 
> 然而极致的意思就是准确率100%，我下面再试一个正常的，前一天只要浏览就交的测试，F1值低的令人发指，明显是准确率太低，看的多但买的少。所以这个方案的难点就是在提升准确率。搞定准确率，哪怕搞定一半，成绩都绝对高。
> 
> ```
> 第28天为标签
> 336427 4367
> 0.005968414936882692
>31278 504
> 0.006607513686992638
>第29天为标签
> 328416 4429
>0.0062191109976115
> 31063 553
> 0.008729757085020244
> 第30天为标签
> 322751 4272
> 0.006409335123217632
> 30073 515
> 0.008173139793383027
> ```
> 
> -------------
> 
> 把浏览记录提升到前两天而非前一天，照样是依照标签数据做到100%准确率，结果仍然有所提升，但是数据量必然是大涨。所以这里我们面临的问题就是如何权衡准确率和召回率，需要好好思考一下，究竟是以前一天数据为准还是以前两天数据为准。按照这个思路，天数越多，每个用户的标签也就越多，虽然召回率上升，但正负样本量必然越不平衡，保持高准确率也会越困难。
> 
> ```
> 第28天为标签
> 1297 4367
> 0.4555084745762712
> 144 504
> 0.4444444444444445
> 
> 第29天为标签
> 1315 4429
> 0.4564763231197771
> 166 553
> 0.4617524339360222
> 
> 第30天为标签
> 1299 4272
> 0.4652665589660744
> 151 515
> 0.4534534534534535
> ```
> 
> --------------
> 
> 最后我们做绝一点，直接把每个用户的所有的浏览记录都交上去，并开挂保持100%准确率。看得出来这就是这个方案的极限，也就是0.6左右的F1值。
> 
> ```
> 第28天为标签
> 2097 4367
> 0.6271658415841584
> 226 504
> 0.6191780821917808
> 
> 第29天为标签
> 2076 4429
> 0.6164488854727133
> 252 553
> 0.6236024844720497
> 
> 第30天为标签
> 2043 4272
> 0.6242280285035628
> 234 515
> 0.6221628838451269
> ```
> 
> ----
> 
> 所以，我们也可以得出结论，用户除了会买自己浏览过的商品外，还有大约四成的人会突然买一些自己前一天及之前从未浏览的商品。这种神经病购买说实话，难顶，直接放弃，太难预测了。我们的主要精力还是要放在可以预测的，也就是用户有征兆的去购买的行为上。

```
2019/12/28 01:53
```

> 开始构建_A方案的特征提取函数，目前预设了以下特征，暂时还未敲完所有代码，因为敲了N多BUG在不停调试，干。
>
> ```
> 特征方案: _A_01
> 第一部分: 用户的特征
>     # 用户在 全集中的/子集中的 总/前1天内/前2天内/前3天内  浏览/收藏/购物车/购买 的计数
>     # 用户在 全集中/子集中 浏览/收藏/购物车/购买 过几种商品
>     # 用户在 全集中的/子集中的 转化率
>     
> 第二部分: 商品的特征
>     # 商品在/商品种类在 总/前1天内/前2天内/前3天内浏览/收藏/购物车/购买 的计数
>     # 商品的/商品种类的 购买/收藏 的计数在 全集/子集 中的 正/反 排序
>     # 商品的/商品类型的 转化率(浏览X购买)
> 
> 第三部分: 用户X商品 的特征
>     # 用户是否 收藏/购买 过这个商品
>     # 用户 总共/前1天当天/前2天当天/前3天当天 购买了/浏览了 几次这个商品
>     # 用户多久前收藏这个商品
>     # 用户与这个商品最后一次交互 是购买1 还是收藏1.5 还是浏览2 还是加购物车4
>     # 商品在 全集/子集 是否是用户最后的交互对象
>     # 商品是用户在 全集/子集 倒数第几个交互对象
>    ```
>    
> 目前的思路是由前两部分特征判断用户购买的欲望、商品被出售的概率，再由三四部分特征判断用户单独对于这个商品的购买意向，实际效果估计明天就能看到了。
>   
>    但是说实话，由于这是初步方案，特征太过于庞杂了。目前我希望之后直接把一二部分由模型预测 用户当天是否买、商品当天是否卖 的概率当做特征，再加进三四部分进行判断，也就是很早以前提到的迁移学习全集中的内容。而三四部分特征是重点，因为用户买不买什么的只是第一阶段，用户买不买指定的某个商品才是重点。

```
2019/12/28 22:23
```

> 写完了昨天所说的所有特征。
>
> ----
>
> 另外提交了有史以来的第一个结果，也就是把标签前一天的所有加购物车的商品交了上去，F1值0.058，有点高啊，是不是我验证集的F1求取方法出了问题导致偏低... ... 
>
> -----
>
> 另外继续完善了方案_A，随手做出了第一个验证集。
>
> 1. 以第二十四天为验证集标签，第二十三天为训练集标签
>
> 2. 用前14天，也就是两周的时间按 ```特征方案: _A_01``` 提取特征
>
> 3. 取预测概率前700视作会购买
>
> 4. 单XGBOOST模型
>
> 5. params = {    
>   'objective': 'binary:logistic',        
>   'eta': 0.05,    
>   'max_depth': 5,    
>   'colsample_bytree': 0.8,    
>   'subsample': 0.8,    
>   'min_child_weight': 16,    
>   }
>   round = 500
>
> 
>6. 验证集成绩，用自己的F1评分为0.06上下。
> 
> 7. 另外要提的是，若删去第一部分第二部分特征其余不变，F1成绩会上升，但AUC成绩会？？也就是说用户和商品单独的特征对于我们的模型是一种累赘，需要解决。
> 
> 8. 再尝试把训练集扩展到22-23天两天的时间为标签，成绩下降，目前原因未知，需要额外验证。 
> 
> 总之，这题没我想的那么轻松（大雾，是这样的，特别是特征还完全没挖，现在只是在靠熟练度堆了一堆而已，现在还只是方案_A。
> 


```
2019/12/29 00:44
```

> 说干就干吧，把用户和商品的特征都迁走，变成用户购买的概率和商品被购买的概率。
>
> 不管其他问题，迁移有无效果，真的还看你迁移数据的质量。稍微调整一下获得迁移数据的模型，最终的效果波动还是肉眼可见的有变化。
>
> 另外悲号，因为迁移动了原代码，还原0.06的成绩出问题了，明早起来再说吧，人裂开了。

```
2019/12/29 15:31
```

> 关于昨晚不能复现成绩的问题，问题找到了，一个小参数在不起眼的地方传错了。
>
> 这一条我只是想记录下什么东西花了我半个白天。

```
2019/12/29 21:59
```

> 首先我想吐槽一下啊，官方评分时间过去一个小时了，依然没看到结果，这是周日要放假吗... ...
>
> -----
>
> 另外学到了xgb的一个新的训练参数 [tree_method](https://xgboost.apachecn.org/#/docs/15)<sup>[5]</sup> , 应该是在构建树时用的一个参数，默认为 ```auto``` ，即数据量大时自动用 ```hist``` 直方图近似训练，反之用 ```exact``` 精确训练。好了，一听我们就知道该怎么写了。 
>
> .....
>
> 涨分明显，舒服了。
>
> ------
>
> 内存99%是常态，现在还根本没开始叠训练集，也没扩充特征，不想做的时候该优化下代码了。
>
> ------
>
> 正在手撸_B方案，准备按深度学习做。
>
> 虽然这种 用户X巨多商品 的模式不太友好，但是按照很久前的思路，可以把深度学习的结果做成一种"意向"类的判断... 有点抽象，我也解释不太清楚，之后看看效果再说，按照一天或者一周的序列做RNN。先做一天的，因为完全不需要提特征。
```
2019/12/31 00:22
```
> 看见一个[可参考的方案](https://zhuanlan.zhihu.com/p/69511536)<sup>[8]</sup>，结合[代码](https://www.kaggle.com/victorykim/ali-buy-predict-data)<sup>[9]</sup>可知这位老哥是按照"用户商品有过任何交互就算待预测"的思路做。我还是先只参考下特征，结合一下自己的思考更新 _A_02 版特征，
>
> 添加了：
>
> 1. ```用户 总共/前1天当天/前2天当天/前3天当天 收藏/加购物车 了几次这个商品```
> 2. ```商品/商品类型 被多少人 浏览/收藏/购物车/购买 过```
> 3. ```用户最后一次 浏览/收藏/购物车/购买 距标签多少小时```
> 4. ```用户最后一次 浏览/收藏/购物车/购买 该商品/该商品类型 距标签多少小时```
> 9. ```用户有几个经纬度```
> 10. ```商品/商品类型 有几个经纬度```
> 
> 删除了：
> 
> 1. 用户多少天前收藏这个商品
>
> ```
>特征方案: _A_02
> 第一部分: 用户的特征
># 用户在 全集中的/子集中的 总/前1天内/前2天内/前3天内  浏览/收藏/购物车/购买 的计数
> # 用户在 全集中/子集中 浏览/收藏/购物车/购买 过几种商品
> # 用户在 全集中的/子集中的 转化率
> # 用户最后一次 浏览/收藏/购物车/购买 距标签多少小时
> # 用户有几个经纬度
> 
> 第二部分: 商品的特征
> # 商品在/商品种类在 总/前1天内/前2天内/前3天内浏览/收藏/购物车/购买 的计数
> # 商品的/商品种类的 购买/收藏 的计数在 全集/子集 中的 正/反 排序
> # 商品的/商品类型的 转化率(浏览X购买)
> # 商品/商品类型 被多少人 浏览/收藏/购物车/购买 过
> # 商品/商品类型 有几个经纬度
> 
> 第三部分: 用户X商品 的特征
> # 用户是否 收藏/购买 过这个商品
> # 用户 总共/前1天当天/前2天当天/前3天当天 浏览/收藏/加购物车/购买 了几次这个商品
> # 用户与这个商品最后一次交互 是购买1 还是收藏1.5 还是浏览2 还是加购物车4
> # 商品在 全集/子集 是否是用户最后的交互对象
> # 商品是用户在 全集/子集 倒数第几个交互对象
> ```
> -----
> 
> 另外还有个新东西我比较感兴趣，就是 [xgoost自定义损失函数和评价函数](https://blog.csdn.net/lujiandong1/article/details/52791117)<sup>[10]</sup> ，我挺想试试自己定义一下损失函数的效果。
> 
> -----
> 
> 还有个这个赛题的东西，因为目前只选择了 浏览->购买 的预测，那还有不浏览直接购买的呢？可以尝试做一个互补的模型看看效果。

```
2019/12/31 21:41
```

> 今年最后一天，现在我正在边看B站跨年晚会边改代码写日记，现在是 《钢铁洪流进行曲》，哒哒哒哒哒哒~~ 气突华~~~
>
> 嗯... 还是说正事，_A_02版特征还差几个，虽然代码敲完了，但是run起来花的时间太久了，我还在想想怎么优化下代码。其他增加的特征，根据今天的线上测试结果看得出来，上分还是有效的，但是离前面大部队还是有很大差距。
>
> 我初步估计下，我的特征繁杂，这个问题不多说了，本来就是一直在强调的，等时间合适了我会开始删减特征。另外是recall的问题，我现在按前几天浏览历史去预测的方案，recall的极限只有0.4-0.5，我已经尝试了直接把所有有交互的历史全部拿来预测，验证集依然会张，所以这种直接增加recall的方法是有效的。这种方法相互补的另一半，也就是相对应的 不浏览就买和当天浏览当天买，这部分的recall可也有0.5上下，能否想办法解决一下，是否可以做成两个模型互补。其余的至于调参或者融合这些，等实在没法提分了再整。
>
> 至于还有个深度模型，在做了在做了（1% ... ...
>
> 另外优化了一些提特征的流程。

```
2020/01/01 16:06 
```

> 内存炸了啊... 把标签数量提升到 “之前只要有交互就算” 的样子，一个训练集就4G... ... 看来这种包容的提升recall的方法用不得。再删减、再删减、再删减.... 与之前没变化了啊，今天全被这内存问题搞死。
>
> 另外是中午的交的一个迁移学习的尝试失败了，降分明显，所以这个方法暂时先放下，再挖挖特征，还有尝试下xgb的自定义损失函数。

```
2020/01/01 22:11
```

> 今天实在没啥进展，因为内存爆炸在优化代码，虽然最后也没跑成功，只能交俩控制变量的试水结果，有两个结论：
>
> 1. 取概率前900比取前1100好，但不知道继续缩小范围的结果如何
> 2. 尝试把与子集有关的结果删去，结果成绩下降，我本以为子集有关的特征比较多余
>
> ----
>
> 另外还研究了下xgb自定义损失函数的问题，有点意思，[xgb是用损失函数的一阶导数和二阶导数来补梯度残差]( https://www.kdd.org/kdd2016/papers/files/rfp0697-chenAemb.pdf )<sup>[11]</sup>，所以自定义的损失函数需要二阶可导，原来是这意思，我是有点兴趣，可能不会涨分但之后要花时间研究下。

``` 
2020/01/02 11:43
```

> 早上交了两个答案，简单的把 ```取概率前900``` 换成了 ```前700``` ```前500```，涨分还是很明显的，不知道再往下降降会不会有效果，晚上预计再交一个 ``` 前400``` 和 ```取标签前3天的交互``` 作为标签。今天还要继续优化代码，这题想要做高，16G的内存还是有点吃不消。
>
> -------
>
> 我还找到了一个本赛题的 [“一千零一个特征”](https://www.cnblogs.com/brucemengbm/articles/6842918.html)<sup>[12]</sup> ，可以再结合下别人的经验更新下特征。
>
> ----
>
> 创建方案 _C，为方案 _A的后续增强版，主要优化暂定包括：
>
> 1. 特征精简 
> 2. 模型融合
> 3. 代码优化
>
> 而 _A方案之后会主要用于添加特征以及更多的尝试性操作， _C 属于正式的版本，不排除还会有 增增增增强的版本(.... 。

```
2020/01/02 16:41
```

> 更新 基准验证集：
>
> 1. 31取验证集标签，30取训练集标签
> 2. 特征： _A_02 
> 3. 模型：XGBOOST
> 4. 模型参数：XGB_01
> 5. 取 标签前所有日期 为提特征区间
> 6. 取前标签前 一 天所有浏览记录为标签
> 7. 成绩：f1: 0.083265, precision: 0.097328, recall: 0.072753
>
> ---
>
> 发现 ``` 商品是否是用户最后的交互对象``` 特征提取代码写错了，更正之后验证分数上涨。
>
> 今天之前的该特征均用错误代码提取。

```
2020/01/02 20:36
```

> 优化代码时又又又出问题了，现在在排BUG，我佛了。讲道理，重复的代码还是能少就少，不然太容易出BUG，高效减少程序员寿命和发际线。目前在按照验证集基准复原，至于要多久，我也不知道了。 ？？？ 我是真的不知道... ...
>
> 除了排BUG，今晚还交了几个控制变量法的结果，已知往前扩充标签的方法是有用的，也就是取标签前3天比取标签前1天的浏览记录作为预测对象更有效；去所有浏览记录比单独去加购物记录更有效。
>
> ---
>
> 之后还要改一个问题，我现在是把 user_id 和 item_id 拼接在一起使用，一是造成内存浪费，二是在merge和isin，特别是isin函数里，测试后发现有点问题，之后应该会用别的方法替代下如何表示 user-item。
>
> ----
>
> 为每个方案创建了一个代码备份文件夹 ``` copies ```，以后每有一次提交记录，就备份一遍特征代码，别问为什么。

```
2020/01/03 02:35
```

> 啊我必须要记录一下，[Pycharm恢复历史文件](https://blog.csdn.net/candy_gl/article/details/79226536)<sup>[13]</sup> ，差点忘了Pycharm还有这功能了，然后找到凌晨2点也还没找到问题出在哪里？

```
2020/01/03 17:17
```

> 是这样的，``` 商品在 全集/子集 是否是用户最后的交互对象 ``` 这个特征我本来准备优化下提取速度。思路是这样的
>
> | index  | user_id  | item_id          | ui_id                  | time                  |
> | :------- | :------- | :--------------- | :-------------------- | :-------------------- |
> | 1 | xiao | ming | xiaoming |15|
> | 2 | xiao | hua | xiaohua |16|
> 我首先要取离标签最近的一个记录，也就是取time最大的一条标签，即取 index=2 的记录。所以我对time和user_id进行sort：
>
> | index | user_id | item_id | ui_id    | time |
> | :---- | :------ | :------ | :------- | :--- |
> | 2     | xiao    | hua     | xiaohua  | 16   |
> | 1     | xiao    | ming    | xiaoming | 15   |
>
> 然后我进行去重，因为是判断用户的最后交互对象，于是应该是对 user_id去重，目标是：
>
> | index | user_id | item_id | ui_id   | time |
> | :---- | :------ | :------ | :------ | :--- |
> | 2     | xiao    | hua     | xiaohua | 16   |
>
> 然后判断只要在 ui_id 内的记录，就是最后一条交互记录。
>
> 然而，然而，我去重的时候是对 ui_id 去重... ... 这样的后果就是，这条特征全是 1... ... 
>
>  (最后我为什么要取 xiaoming xiaohua 呢？ 我要偷偷安利一部漫画叫做《拜见女王陛下》.. 秋梨膏把它看完吧，不要看几话就弃了...) 
>
> ----
>
> 另外经过一天的折腾，总共发现有几条特征是有问题的， 此时及之前的提交记录都有下面问题，：
>
> ``` 商品/商品类型 被多少人 浏览/收藏/购物车/购买 过 ```
>
> ```商品在/商品种类在 总/前1天内/前2天内/前3天内浏览/收藏/购物车/购买 的计数```
>
> ```用户最后一次 浏览/收藏/购物车/购买 距标签多少小时```
>
> ``` 商品在 全集/子集 是否是用户最后的交互对象 ```

```
2020/01/03 23:11 
```

> 我有点无语，这个官网评测真的有点东西，不知道他内部的评测函数长什么样子。
>
> 我修改了所有有问题的特征，最后得到的文件也不同，交上去居然是和之前有问题的时候一样的分... ... 之前不是有个0分文件吗，按道理说就是准确率必定为零，所以我尝试把结果文件中有0分文件里的记录都删去，结果还降分了？纳尼？那说明我那个0分不应该是0分啊？
>
> 算了算了，在几个特征上纠结了两天，心态都纠结得不好了。
>
> 讲真，如果你纠正了你的错误，反而还降分了，那就不要理会，要坚持正确。用靠运气提分这个思想就是不对的，为了一点蝇头小利就放弃提分的正道，不应该啊。
>
> ----
>
> 另外今晚尝试把30和31的特征叠加起来预测，成绩暴跌，之后暂时不考虑这个方法。
>
> 还尝试用30做标签预测32，也就是隔天预测，依然是降分，说明这方法暂时行不通。
>
> 把取标签扩充到前三天浏览，涨分。继续扩标签当做提分的手段之一。
>
> ------
>
> 明天会开始尝试处理异常天数、XGB自定义损失函数、增加特征。

```
2020/01/04 01:50
```

> 在鼓捣xgb自定义函数的时候我还找到两个很有意思的东西，是我在一篇超详细的[XGBOOST学习]( https://www.biaodianfu.com/xgboost.html)<sup>[14]</sup> 里看见。一个是Dart Booster，但里面关于dart booster的描述应该有错误，按照设计思想是先生成的树更重要  ["in a sense,the First tree learns the bias of the problem while the rest of the trees in the ensemble learn the deviation from this bias."](http://proceedings.mlr.press/v38/korlakaivinayak15.pdf) <sup>[15]</sup>，这是Dart Booster，实现了深度学习中的 drop_out 功能；另一个是自定义eta，也就是我们可以自己实现[学习率的衰减和热重启]( https://arxiv.org/pdf/1608.03983.pdf )<sup>[16]</sup>。以后调参我很有兴趣试试。
>

```
2020/01/04 14:35
```

> xgb可以用gpu计算，通过修改 tree_method 参数使用。这不是问题，问题是 ```'tree_method': 'exact'``` 和 ```'tree_method': 'gpu_exact'``` 的结果为什么会不同...  听说是因为gpu的float精度只有32，但最后的结果差距大的有点超乎我想象... 暂时还是别用gpu吧。

```
2020/01/05 00:58
```

> 今天（啊，第二天的凌晨都算在今天内）记录的有点晚，因为一直在跑模型，但是效果还是有的，现在的分已经在赛题的前20名了。
>
> 首先是昨天提的XGBOOST的 drop_out 与 学习率热重启，都尝试了，在验证集的效果不怎么好。drou_out 我设置的很保守，但是验证集里有稍稍的减分；另外一个的问题在于怎么定义学习率衰减和重启的函数，毕竟这是Tree Boosting的学习率，和神经网络的学习率作用有不同，我对GBDT模型的学习率理解还没透彻，不好去自己定义，单单是把神经网路的余弦退火和热重启拿过来用了一下，效果不好，之后会继续尝试。
>
> 另外一个是按照本赛题自定义了XGBOOST的评测函数(非损失函数)，观察发现可能迭代次数过多了，所以今晚交了减少迭代次数交了答案，分数上涨。
>
> 最后还有一个问题，因为双12有异常是毋庸置疑的，所以我尝试把 只在双12出现的商品和用户 数据删除，但评测结果下降，所以这部分异常暂时先放下，之后找出更合理的办法再处理。
>
> -----
>
> 明天要加特征，还有继续研究XGB自定义损失函数。

```
2020/01/05 15:55
```

> 新版测试特征出台，对比_A_02版特征 添加了：
>
> ```用户在 全集中的/子集中的 转化率(收藏/购物车X购买)```
>
> ```用户在 全集/子集 的历史中一天最多 浏览/收藏/购物车/购买 多少商品/商品种类```
>
> ```用户在 全集/子集  浏览/收藏/购物车/购买 时间(按天)的 方差/偏度/峰度``` 
>
> ```用户在标签前一天浏览时间(按小时)的 偏度/峰度```
>
> ```用户前 三 天是否一直有 浏览/购买 记录```
>
> ```用户 浏览/购买 时间(按小时不去重)的 均值/众数 ```
>
> ```商品的/商品类型的 转化率(收藏/购物车X购买)```
>
> ```商品的/商品类型 在历史中一天最多被多少次 浏览/收藏/购物车/购买```
>
> ```商品的/商品类型 浏览/收藏/购物车/购买 时间(按天)的 方差/偏度/峰度```
>
> ```商品在标签前一天浏览时间(按小时)的 偏度/峰度```
>
> ```商品前 三 天是否一直有 浏览/购买 记录```
>
> ```用户前3天是否对该 商品/商品类型 一直有浏览记录```
>
> ```用户在标签前一天对该 商品/商品类型 浏览时间(按小时不去重)的 偏度/峰度```
>
> ```用户浏览该商品 时间(按小时不去重)的 均值/众数 ```
>
> ```
>_A_03 测试版
> 第一部分: 用户的特征
># 用户在 全集中的/子集中的 总/前1天内/前2天内/前3天内  浏览/收藏/购物车/购买 的计数
> # 用户在 全集中/子集中 浏览/收藏/购物车/购买 过几种商品
> # 用户在 全集中的/子集中的 转化率(浏览/收藏/购物车X购买)
> # 用户最后一次 浏览/收藏/购物车/购买 距标签多少小时
> # 用户有几个经纬度
> # 用户在 全集/子集 的历史中一天最多 浏览/收藏/购物车/购买 多少商品/商品种类
> # 用户在 全集/子集  浏览/收藏/购物车/购买 时间(按天)的 方差/偏度/峰度 
> # 用户在标签前一天浏览时间(按小时)的 偏度/峰度
> # 用户前 三 天是否一直有 浏览/购买 记录
> # 用户 浏览/购买 时间(按小时不去重)的 均值/众数
> 
> 第二部分: 商品的特征
> # 商品在/商品种类在 总/前1天内/前2天内/前3天内浏览/收藏/购物车/购买 的计数
> # 商品的/商品种类的 购买/收藏 的计数在 全集/子集 中的 正/反 排序
> # 商品的/商品类型的 转化率(浏览/收藏/购物车X购买)
> # 商品/商品类型 被多少人 浏览/收藏/购物车/购买 过
> # 商品/商品类型 有几个经纬度
> # 商品的/商品类型 在历史中一天最多被多少次 浏览/收藏/购物车/购买
> # 商品的/商品类型 浏览/收藏/购物车/购买 时间(按天)的 方差/偏度/峰度
> # 商品在标签前一天浏览时间(按小时)的 偏度/峰度
> # 商品前 三 天是否一直有 浏览/购买 记录
> 
> 第三部分: 用户X商品 的特征
> # 用户是否 收藏/购买 过这个商品
> # 用户 总共/前1天当天/前2天当天/前3天当天 浏览/收藏/加购物车/购买 了几次这个商品
> # 用户与这个商品最后一次交互 是购买1 还是收藏1.5 还是浏览2 还是加购物车4
> # 商品在 全集/子集 是否是用户最后的交互对象
> # 商品是用户在 全集/子集 倒数第几个交互对象
> # 用户最后一次 浏览/收藏/购物车/购买 该商品/该商品类型 距标签多少小时
> # 用户前3天是否对该 商品/商品类型 一直有浏览记录
> # 用户在标签前一天对该 商品/商品类型 浏览时间(按小时不去重)的 偏度/峰度
> # 用户浏览该商品 时间(按小时不去重)的 均值/众数
> ```

```
2020/01/05 23:40
```

> 关于偏度和峰度的特征，我个人觉得是有用的，体现了[数据分布的状态](https://zhuanlan.zhihu.com/p/53184516)<sup>[17]</sup>，但是很多比赛都没看别人用过...  因为分别要用三次方四次方计算，所以这俩有关的特征提起来都太慢了，我在商品类的特征中暂时没加。
>
> 另外是偏度与峰度特征怎么提的问题，我以前一直是直接把一个序列扔进函数完事。但是这样可能是有问题的，比如 [3, 4, 5, 6] 与 [4, 5, 6, 7] 的偏度峰度是一样的？所以我现在提这个特征是要结合数据背景提取。比如就上面两个数据，假设这是两个人每周七天买东西的记录，一个人在前五天买，一个人在后五天买，我不能直接把两个序列送进pd.Series.skew，因为计算机不知道我这其实是一周的数据，想象正态分布的图，你不告诉他，你的正态分布的横坐标就只有5天。所以我会在原始数据里再加上一个横坐标基准，比如：
>
> [1, 2, 3, 4, 5] 变成 [1, 2, 3, 4, 5, 6, 7] + [3, 4, 5] =  [1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5] , skew = -0.206
>
> [3, 4, 5, 6, 7] 变成 [1, 2, 3, 4, 5, 6, 7] + [5, 6, 7] =  [1, 2, 3, 4, 5, 6, 7, 3, 4, 5, 6, 7] , skew = -0.468
>
> 因为下面的更负，所以下面的数据在一周七天的凸起更往右。
>
> -------------------
>
> 另外今天新加的特征有点怪，过拟合非常严重，训练集预测自己比 _A_02 版特征的分数高了一倍，但预测验证集分数爆降，观察验证集的最高分数也不如之前... ... 咋回事啊，哪些特征过拟合得这么严重... ... 暂时先不用这版特征算了。

```
2020/01/06 23:00
```

> 今天上LighGBM了，顺便记录一下[LGB的文档](https://lightgbm.apachecn.org/#/)<sup>[18]</sup>。
>
> 快是快啊，但我用lgb始终不顺手，虽然验证集调的和xgb一样甚至超过的分数，但线上成绩会下降很多；而我xgb基本上能保证线上线下一致... 哎以后慢慢学吧。
>
> ----
>
> 另外一个我今天确定了一个事情，线上的评测真有点问题，我调整了一些东西，很多结果，交到线上都是一个成绩，包括顺手搞的一个线性加权的融合，分还是不变？结合之前那个0分成绩，不哆嗦了。
>
> -----------
>
> 还有新版特征为什么还没交过线上，因为验证集上的表现太一言难尽，非常容易过拟合，我还在想办法解决，看能不能删减一些。

```
2020/01/07 11:14
```

> 特征方面我是不想再挖了，只剩下精简这一个手段，但是费时费力。
>
> 模型的话，这题难，特别是模型融合，单纯的线性加权概率，测试过，效果不怎么好。
>
> 其余的像继续扩充标签等操作，我内存真搞不定了。
>
> 常规方法没得大变动的话，那就上深度学习了。
>
> 之后再回来加细节，什么调参特征归一化之类的，甚至睡几天会突然发现什么解题的奇淫技巧也说不定。

```
2020/01/07 23:31
```

> 尝试了一天的RNN，失败。为什么？我猜测是正负样本差距太大太大太大，当然也有我功力不够的问题。哎，瓶颈期了啊，不玩骚操作的话就有点山穷水尽的感觉了。

```
2020/01/09 15:36 
```

>感冒摸了两天鱼，我也是第一次知道喝可乐可以缓解头昏脑涨（因为可乐里有咖啡因。
>
>回到了最原始的上分方法：加特征。
>
>写了一堆特征包括
>
>``` 
># 用户在双12 1/2/3/4 了多少次
># 用户每天 平均/最多 1234 多少
># 用户最早出现在多久
># 用户标签前 三 天是否一直有 浏览/购买 记录
># 商品在双12 1/2/3/4 了多少次
># 商品每天 平均/最多 1234 多少
># 商品最早出现在了多久
># 商品前 三 天是否一直有 浏览/购买 记录
># 用户之间商品 最小 经纬度
># 全集内/子集内 商品在用户 浏览/购买 计数数中的 正/反 排序
># 用户前3天是否对该 商品/商品类型 一直有浏览记录
># 用户 最早 和这个商品/商品类型 的交互时间
>```
>根据验证集中的表现只留下了 ```全集内/子集内 商品在用户 浏览/购买 计数数中的 正/反 排序```  和  ```用户前3天是否对该 商品/商品类型 一直有浏览记录```  ，提交之后涨分明显，目前已经 0.11的分了，官方排名前10。我看看，如果想要冲到前5，还有0.004的差距，看起来很小，但不知道要做多久，可能一个星期，也可能就换个随机种子的事，也要看看能不能运气好再找到几个有用的特征。讲真，这种排序特征，每次在特征重要性里面得分都不高，但是上分就是特别有效，绝了。
>
>---------
>
>另外一个是我在尝试xbg的两个参数，分别是 ``` max_delta_step ``` 和 ```scale_pos_weight``` 用于解决类别不平衡的问题。根据[xgb的英文文档里的原话]( https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html# )<sup>[19]</sup>：
>
>```
>For common cases such as ads clickthrough log, the dataset is extremely imbalanced. This can affect the training of XGBoost model, and there are two ways to improve it.
>
>- If you care only about the overall performance metric (AUC) of your prediction
>     - Balance the positive and negative weights via `scale_pos_weight`
>      - Use AUC for evaluation
> - If you care about predicting the right probability
>     - In such a case, you cannot re-balance the dataset
>      - Set parameter `max_delta_step` to a finite number (say 1) to help convergence
> ```
>
>
>可知在追求高auc的时候，可以选择 scale_pos_weight ，在追求高准确率的时候可以选择调整 max_delta_step。我就很难受了，调整哪个都不行，验证集上还好，官网评测就降分，算了算了。
>
>------
>
>我现在还在尝试[特征选择](https://www.cnblogs.com/hhh5460/p/5186226.html)<sup>[20]</sup>，目前只用了递归特征消除(Recursive feature elimination)，效果还是一言难尽，验证集上的表现降分居多。所以特征一旦提了出来，就再也不好删除了，不要瞎凑一大堆特征，不然删都删不完。
>
>------
>
>还有一件大事，我发现又有一个特征提错了  ``` 商品是用户在 全集/子集 倒数第几个交互对象``` ，这不是重点，重点是修改成为正确的特征之后，降分了？？？而且降得很多，哦，我裂开了，太真实了。现在看来的话，我只能先尽力冲到前5，再回头把这个错误特征解决了。（之前才说走正道，现在又成分奴了，啧... ....
>
>---------------
>
>关于上分还有一件大事，就是模型融合的问题。本来模型融合应该是上分的最有效的战略武器之一，但是这题就怪糟糟的，无论你是线性加权，还是我才做的stacking，基本都涨不了分，甚至还能降到 0%以下... ... 是的有百分号的零。再说融合的基模型，特别是线性模型，像是线性回归和逻辑回归的效果，都非常差，难以参加融合，只能树模型这种决策模型能稍稍一用。
>
>我们先静下来理一下思路：
>
>1. 假设有三个相互独立的弱学习器的正确率为0.7，那三个弱学习器采用投票(即二比一即以上)策略的正确率等于 0.7^3+0.7x0.7x0.3x3 = 0.343+0.441=0.784 ，高于单一的0.7，这是模型融合的基本原理。
>2. 假设有三个弱学习器正确率分别为 [0.6, 0.6, 0.7]，投票正确率同理为 0.696 。 这时候问题就来了啊，就算有三个模型，但是你融合之后的效果还不如一个最好的基学习器。
>3.  回到我们本赛题，巨大的问题，我们构建的模型，正确率甚至低到0.1左右，假设我们有三个正确率皆为0.1的弱学习器，那么投票融合之后的正确率等于 0.1^3+0.1x0.1x0.9x3 = 0.001 + 0.027 = 0.028 ？？？我融合的是个锤子呢 ... ...
>
>我想这就是模型融合为何难以提分的原因了，但毕竟上述条件实现在各个模型相互独立的情况之下，所以，完全放弃模型融合，我目前还是难以接受的。
>
>顺便趁着还在跑程序，我算了算，x^3 + x^2 * (1-x) * 3 > x ，x 必须要在 0.5以上。嗯？怎么感觉不用算也想得出来的...

```
2020/01/10 09:23
```

> 对不起我昨天沙比。
>
> 我昨天说模型融合的单模型准确率要高于0.5，而我自己做的模型准确率只有0.1所以不能融合... 个铲铲，我自己做的模型的0.1准确率是针对取的前N个样例，后面的根本没有计算，所以我的模型并不是真的只有0.1准确率。我们再回顾下什么叫做"0.5的准确率"，说白了就是你的模型起码要比随机的好，不然我乱说一通比你做半天的效果好，那才是真的叫不如不做。那我们这道题，随机情况下的F1值按他的要求有多少呢？我自己写了个小程序暴力法迭代了几万次，F1值不到0.005，而我们做的F1值有0.1上下，所以我们的模型是有效的，比随机的效果要好，再所以，模型融合在理论上是有效果的！
>
> 啊，昨晚跑了个0.1135的双模型简单投票（同取两个模型的概率前N的交集，按照500个左右定N的大小）的结果，高于单模型，舒服了，还差0.0006就前5。既然模型融合在理论和实际上都有效，那我就再多跑几个。
>
> 还有个问题，概率直接线性加权融合的效果不如直接投票，实践也如此。
>
> ---
>
> 我准备再上一个 [CatBoost]( https://catboost.ai/docs/ )<sup>[21]</sup>, 再记录下一个[相关的中文介绍]( https://www.biaodianfu.com/catboost.html )<sup>[22]</sup> 。但问题是这东西适合离散特征，而我原有的特征完全是盯着XGB这种cart二叉树来提的... 看最后的效果吧，再为一个模型提一组新特征就太累了...

```
2020/01/11 11:04
```

> 好了进前五了，说说最后是怎么做的。
>
> 首先是一个CatBoost的单模型，在验证集上调调参，再提交两个结果，分数有个0.096将就拿来用了。按照之前的vote方法和xgb、lgb的最优模型融合，发现行不通，因为cb这个模型的分数较前俩有些低了，融合之后甚至还降分。
>
> 这时候我们干脆换个思路，既然你cb这个模型参与正常融合的效果太弱，那我干脆就只把你当成一个"守门员"。什么意思？排名前500的武林高手你打不过，我体谅你，但是排名5000的小混混你要是还打不过（评判不出对错），我就无法理解了哦？好歹你也是有0.096分的。所以我第一步照常让 xgb 和 lgb 正常投票取前N个交集；第二步，把这个结果与cb的概率前 10N 再取交集，如果有的结果连cb的前 10N 都进不了，那就say bye bye。
>
> 而10N好还是5N好还是其他，根据模型好坏决定吧。
>
> 这个项目暂时就做到这里吧，提分空间肯定还很大，特别是特征上，我基本是一套特征加加减减全程做完，没有深挖，也没有处理异常、没有扩充训练数据没有平衡正负样本等等。



#### 提交记录

<sup>特别提示：有些特征可能提取时含BUG，具体问题详见对应 [特征方案](特征方案)，根据提交时间和方案内的BUG修改时间进行核对。

 略 .



## 方案保存

#### 特征方案

> ##### _A_01
> ```
> 第一部分: 用户的特征
> # 用户在 全集中的/子集中的 总/前1天内/前2天内/前3天内  浏览/收藏/购物车/购买 的计数
> # 用户在 全集中/子集中 浏览/收藏/购物车/购买 过几种商品
> # 用户在 全集中的/子集中的 转化率
> 
> 第二部分: 商品的特征
> # 商品在/商品种类在 总/前1天内/前2天内/前3天内浏览/收藏/购物车/购买 的计数
> # 商品的/商品种类的 购买/收藏 的计数在 全集/子集 中的 正/反 排序
> # 商品的/商品类型的 转化率(浏览X购买)
> 
> 第三部分: 用户X商品 的特征
> # 用户是否 收藏/购买 过这个商品
> # 用户 总共/前1天当天/前2天当天/前3天当天 购买了/浏览了 几次这个商品
> # 用户多久前收藏这个商品
> # 用户与这个商品最后一次交互 是购买1 还是收藏1.5 还是浏览2 还是加购物车4
> # 商品在 全集/子集 是否是用户最后的交互对象
> # 商品是用户在 全集/子集 倒数第几个交互对象
> ```
> ##### 历史BUG
>
> ```
> 2020/01/03 18:29 修改:
> 1. 商品/商品类型 被多少人 浏览/收藏/购物车/购买 过
> 2. 商品在/商品种类在 总/前1天内/前2天内/前3天内浏览/收藏/购物车/购买 的计数
> 3. 用户最后一次 浏览/收藏/购物车/购买 距标签多少小时
> 4. 商品在 全集/子集 是否是用户最后的交互对象
> 
> 2020/01/09 13:05 修改：
>  1. 商品是用户在 全集/子集 倒数第几个交互对象
> ```



> ##### _A_02
>
> ```
> 第一部分: 用户的特征
> # 用户在 全集中的/子集中的 总/前1天内/前2天内/前3天内  浏览/收藏/购物车/购买 的计数
> # 用户在 全集中/子集中 浏览/收藏/购物车/购买 过几种商品
> # 用户在 全集中的/子集中的 转化率
> # 用户最后一次 浏览/收藏/购物车/购买 距标签多少小时
> # 用户有几个经纬度
> 
> 第二部分: 商品的特征
> # 商品在/商品种类在 总/前1天内/前2天内/前3天内浏览/收藏/购物车/购买 的计数
> # 商品的/商品种类的 购买/收藏 的计数在 全集/子集 中的 正/反 排序
> # 商品的/商品类型的 转化率(浏览X购买)
> # 商品/商品类型 被多少人 浏览/收藏/购物车/购买 过
> # 商品/商品类型 有几个经纬度
> 
> 第三部分: 用户X商品 的特征
> # 用户是否 收藏/购买 过这个商品
> # 用户 总共/前1天当天/前2天当天/前3天当天 浏览/收藏/加购物车/购买 了几次这个商品
> # 用户与这个商品最后一次交互 是购买1 还是收藏1.5 还是浏览2 还是加购物车4
> # 商品在 全集/子集 是否是用户最后的交互对象
> # 商品是用户在 全集/子集 倒数第几个交互对象
> # 用户最后一次 浏览/收藏/购物车/购买 该商品/该商品类型 距标签多少小时
> ```
> ##### 历史BUG
> ```
> 2020/01/03 18:29 修改:
> 1. 商品/商品类型 被多少人 浏览/收藏/购物车/购买 过
> 2. 商品在/商品种类在 总/前1天内/前2天内/前3天内浏览/收藏/购物车/购买 的计数
> 3. 用户最后一次 浏览/收藏/购物车/购买 距标签多少小时
> 4. 商品在 全集/子集 是否是用户最后的交互对象
> 
> 2020/01/09 13:05 修改：
>  1. 商品是用户在 全集/子集 倒数第几个交互对象
> ```

> ##### _A_03
>
> ```
> 第一部分: 用户的特征
> # 用户在 全集中的/子集中的 总/前1天内/前2天内/前3天内  浏览/收藏/购物车/购买 的计数
> # 用户在 全集中/子集中 浏览/收藏/购物车/购买 过几种商品
> # 用户在 全集中的/子集中的 转化率(浏览/收藏/购物车X购买)
> # 用户最后一次 浏览/收藏/购物车/购买 距标签多少小时
> # 用户有几个经纬度
> # 用户在 全集/子集 的历史中一天最多 浏览/收藏/购物车/购买 多少商品/商品种类
> # 用户在 全集/子集  浏览/收藏/购物车/购买 时间(按天)的 方差/偏度/峰度 
> # 用户在标签前一天浏览时间(按小时)的 偏度/峰度
> # 用户前 三 天是否一直有 浏览/购买 记录
> # 用户 浏览/购买 时间(按小时不去重)的 均值/众数
> 
> 第二部分: 商品的特征
> # 商品在/商品种类在 总/前1天内/前2天内/前3天内浏览/收藏/购物车/购买 的计数
> # 商品的/商品种类的 购买/收藏 的计数在 全集/子集 中的 正/反 排序
> # 商品的/商品类型的 转化率(浏览/收藏/购物车X购买)
> # 商品/商品类型 被多少人 浏览/收藏/购物车/购买 过
> # 商品/商品类型 有几个经纬度
> # 商品的/商品类型 在历史中一天最多被多少次 浏览/收藏/购物车/购买
> # 商品的/商品类型 浏览/收藏/购物车/购买 时间(按天)的 方差/偏度/峰度
> # 商品在标签前一天浏览时间(按小时)的 偏度/峰度
> # 商品前 三 天是否一直有 浏览/购买 记录
> 
> 第三部分: 用户X商品 的特征
> # 用户是否 收藏/购买 过这个商品
> # 用户 总共/前1天当天/前2天当天/前3天当天 浏览/收藏/加购物车/购买 了几次这个商品
> # 用户与这个商品最后一次交互 是购买1 还是收藏1.5 还是浏览2 还是加购物车4
> # 商品在 全集/子集 是否是用户最后的交互对象
> # 商品是用户在 全集/子集 倒数第几个交互对象
> # 用户最后一次 浏览/收藏/购物车/购买 该商品/该商品类型 距标签多少小时
> # 用户前3天是否对该 商品/商品类型 一直有浏览记录
> # 用户在标签前一天对该 商品/商品类型 浏览时间(按小时不去重)的 偏度/峰度
> # 用户浏览该商品 时间(按小时不去重)的 均值/众数
> ```

#### 模型参数

> #### XGB_01
>
> ```python
> params = {       
> 	'objective': 'binary:logistic',       
> 	'eta': 0.05,        
> 	'max_depth': 5,       
>        'colsample_bytree': 0.8,       
>        'subsample': 0.8,      
> 	'min_child_weight': 16, 
>        'tree_method': 'exact',
>    } num_round=500
> ```

> #### LGB_01
>
> ```python
> params = {
>     'task': 'train',
>     'objective': 'binary',
>     'learning_rate': 0.05,
>     'boosting': 'gbdt',
>     'min_child_weight': 20,
>     'feature_fraction': 0.8,
>     'bagging_fraction': 0.8,
>     'bagging_freq': 5,
>     'num_leaves': 50,
>     'verbose': -1,
> }num_round=300
> ```

>  #### LGB_02
>
> ```python
> params = {
>        'task': 'train',
>        'objective': 'binary',
>        'learning_rate': 0.01,
>        'boosting': 'gbdt',
>        'min_child_weight': 24,
>        'feature_fraction': 0.9,
>        'bagging_fraction': 0.8,
>        "reg_alpha": 0.3,
>        'num_leaves': 50,
>        'verbose': -1,
> }num_round=300
> ```

>  #### CB_01
>
>  ```python
>  (iterations=num_round,
>   depth=6,
>   learning_rate=0.01,
>   loss_function='CrossEntropy',
>   subsample=0.8,
>   bootstrap_type='Bernoulli',
>   l2_leaf_reg=0.5, allow_const_label=True, leaf_estimation_method='Newton')
>  num_round=500
>  ```



#### 验证集基准

```
1. 24取验证集标签，23取训练集标签
2. 特征： _A_01 取第三部分
3. 模型：XGBOOST
4. 模型参数：XGB_01 除去 tree_method: exact
5. 取 标签前14天 为提特征区间
6. 取前标签前 一 天所有浏览记录为标签
7. 取预测概率前 700 视作会购买
8. 成绩：f1: 0.05 左右
```

```
1. 24取验证集标签，23取训练集标签
2. 特征： _A_02 取第三部分
3. 模型：XGBOOST
4. 模型参数：XGB_01
5. 取 标签前所有日期 为提特征区间
6. 取前标签前 一 天所有浏览记录为标签
7. 取预测概率前 700 视作会购买
8. 成绩：0.073394, precision: 0.089613, recall: 0.062147
```

```
1. 31取验证集标签，30取训练集标签
2. 特征： _A_02 
3. 模型：XGBOOST
4. 模型参数：XGB_01
5. 取 标签前所有日期 为提特征区间
6. 取前标签前 一 天所有浏览记录为标签
7. 取预测概率前 700 视作会购买
8. 成绩：f1: 0.091429, precision: 0.106870, recall: 0.079886
```



## 参考文献

[1]. 懒惰de小蜜蜂.Python3安装geohash[EB/OL].[2019/11/26].https://blog.csdn.net/kdyyh/article/details/73694482.

[2]. sighsmile.Geohash 原理及 Python 实现[EB/OL].[2019/11/26].http://sighsmile.github.io/2017-06-30-geohash-python/.

[3]. 薛瑄.base家族：base16、base32和base64，转码原理[EB/OL].[2019/11/26].https://blog.csdn.net/xx326664162/article/details/78122139.

[4]. caoye_oo0.markdown 使用mermaid语法画流程图[EB/OL].[2019/12/14].https://blog.csdn.net/xc_oo0/article/details/97126647.

[5]. 那伊抹微笑,Peppa,1266,腻味.XGBoost 中文文档[EB/OL].[2019/12/15].https://xgboost.apachecn.org/#/.

[6]. Liang Yang, Xi-Zhu Wu, Yuan Jiang, Zhi-Hua Zhou.Multi-Label Learning with Deep Forest[J].arXiv, 2019(1911.06557).

[7]. Xihan Li.简单粗暴 TensorFlow 2.0 | A Concise Handbook of TensorFlow 2.0[EB/OL].[2019/12/15].https://tf.wiki/.

[8]. VictoryJ.阿里天池新人实战赛—移动推荐算法[EB/OL].[2019/12/31].https://zhuanlan.zhihu.com/p/69511536.

[9]. VictoryKim.Ali_buy_predict_data[EB/OL].[2019/12/31].https://www.kaggle.com/victorykim/ali-buy-predict-data.

[10]. BYR_jiandong.xgboost cross_validation&自定义目标函数和评价函数&base_score参数[EB/OL].[2019/12/31].https://blog.csdn.net/lujiandong1/article/details/52791117.

[11]. XGBoost: A Scalable Tree Boosting System. T. Chen, C. Guestrin (2016). A Highly Efficient Gradient Boosting Decision Tree. Guolin Ke (2017). Introduction to Boosted Trees. T. Chen 

[12]. brucemengbm.【天池竞赛系列】阿里移动推荐算法思路解析[EB/OL].[2020/01/02].https://www.cnblogs.com/brucemengbm/articles/6842918.html.

[13]. Candy_GL.误删除pycharm项目中的文件，如何恢复？[EB/OL].[2020/01/02].https://blog.csdn.net/candy_gl/article/details/79226536.

[14]. 标点符.机器学习算法之XGBoost[EB/OL].[2020/01/04].https://www.biaodianfu.com/xgboost.html.

[15]. K.V. Rashmi and R. Gilad-Bachrach. 2015. Dart: Dropouts meet multiple additive

regression trees. Journal of Machine Learning Research 38 (2015).

[16]. Loshchilov I, Hutter F. Sgdr: Stochastic gradient descent with warm restarts[J]. arXiv preprint arXiv:1608.03983, 2016. 

[17]. 胡卫雄.偏度与峰度的正态性分布判断[EB/OL].[2020/01/05].https://zhuanlan.zhihu.com/p/53184516.

[18]. 那伊抹微笑.LightGBM 中文文档[EB/OL].[2020/01/06].https://lightgbm.apachecn.org/#/.

[19]. xgboost developers.XGBoost Documentation[EB/OL].[2020/01/09 ].https://xgboost.readthedocs.io/en/latest/tutorials/param_tuning.html#.

[20]. 罗兵.结合Scikit-learn介绍几种常用的特征选择方法[EB/OL].[2020/01/09].https://www.cnblogs.com/hhh5460/p/5186226.html.

[21]. Andrey Gulin.Overview of CatBoost[EB/OL].[2020/01/10].https://catboost.ai/docs/.

[22]. 标点符.机器学习算法之Catboost[EB/OL].[2020/01/10].https://www.biaodianfu.com/catboost.html.
